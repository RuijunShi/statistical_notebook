\section{Metropolis-Hastings采样}

\subsection{M-H采样原理}

上一章我们讲了MCMC抽样的一些问题，这一章我们介绍MCMC的一种代表算法。这一小节我们介绍M-H采样的原理。

我们需要构造一条马尔可夫链；要构造一个转移核，使得平稳分布就是我们要的抽样分布。参考第二章2.2节的细致平衡：

\[p_{ji}\pi_{j}=p_{ij}\pi_{i}\]

当然要构造这个细致平衡条件很难。假设\textbf{目标分布}为\(\pi(x)\)，转移核为\(q(i,j)\)，通常我们只能得到这样的结果：

\[\pi(i)q(i,j) \neq \pi(j) q(j,i)\]

因此我们要构造一个分布能使得其分布是细致平衡。

假设我们通过建议分布\(q(i,j)\)中随机抽取一个\textbf{候选状态\(x_j\)(后面简写为\(j\))}，我们可以在两端乘以一个\(\alpha(i,j)\)使得其变为平稳分布，即：

$$\pi(i)q(i,j)\alpha(i,j) = \pi(j)q(j,i)\alpha(j,i)$$

在这里\(\alpha(i,j)\)为\textbf{接受分布}。建议分布\(q(i,j)\)是马尔可夫链转移核，且该马尔可夫链是不可约的，同时这个分布是容易采样的。那这个接受分布怎么构建呢？

我们对\( \pi(i)q(i,j)\alpha(i,j) = \pi(j)q(j,i)\alpha(j,i) \) 移项：

\[\alpha(i,j) =\frac{\pi(j)(j,i)}{\pi(i)q(i,j)}\alpha(j,i)\]

实际上我们需要把两边的接受分布扩大到1，这样接受率才会达到最大。如果\(\alpha(j,i)=1\)，有：

\[\alpha(i,j) =\frac{\pi(j)q(j,i)}{\pi(i)q(i,j)}\]

相反的，如果上式\(\alpha(i,j)>1\)，我们令\(\alpha(i,j)=1\)：

\[1=\frac{\pi(j)q(j,i)}{\pi(i)q(i,j)}\alpha(j,i)\]

即我们取\(\alpha(i,j)=1\)。这时候接受分布\(\alpha\)扩大到最大。因此得到接受分布：

\[\alpha(i,j)=\min \left\{1,\frac{\pi(j)q(j,i)}{\pi(i)q(i,j)} \right\}\]

这时候的接受率最高，且容易证明这个转移核\(q(i,j)\alpha(i,j)\)是平稳分布的。之后我们从区间\((0，1)\)中均匀采样，得到一个随机数\(u\)，按照以下判断：

\[x_t=
\left\{
\begin{aligned}
x_j,\ \ u\leq\alpha(i,j) \\
x_i, \ \ u  > \alpha(i,j)
\end{aligned}
\right.\]

决定其是否接受下一步。

怎么理解这个接受分布呢？前面提到，当我们从状态\(i\)以概率\(p(i,j)\)转移到状态\(j\)的时候，不一定是平稳的，而加入\(\alpha\)之后就可以得到一个新的平稳的马尔可夫链。而我们是否要转移到下一步就是要考虑这个接受分布了。我们定性的解释这个问题。假设我们的建议分布\(q(i,j)=q(j,i)\)，我们的接受分布\(\alpha=\min\{1,\frac{\pi_j}{\pi_i}\}\)，如果\(\alpha\)太小，说明我们下一步抽取的\(j\)所对应的概率是远小于我们上一步抽取的概率\(i\)，所以要舍弃；而如果是1的话说明抽取的\(j\)很符合我们的目标分布\(\pi\)，因此可以更好地接近我们要抽样的分布。这个判断步骤类似于本文开头的拒绝-接受采样。\(\alpha\)类似于接受-拒绝采样的\(\frac{f(x)}{c*q(x)}\)。综上，其实这个转移核是要根据我们的抽样\(i,j\)共同确定的。
\subsection{M-H采样算法}

\begin{figure}
\centering
%\includegraphics{http://cos.name/wp-content/uploads/2013/01/mcmc-algo-2.jpg}
\caption{}
\end{figure}
常见的采样有ptmcmc\cite{justin_ellis_2017_1037579}

\section{吉布斯采样}

\subsection{满条件分布}

MCMC要抽样的函数一般都是多变量的联合概率分布\(p(x)=p(x_1,x_2,\cdots,x_k)\)，其中\(x=(x_1,x_2,\cdots,x_k)^T\)是\(k\)维变量。若条件概率分布：\(p(x_I|x_{-I})\)中出现了所有的变量\(k\)，其中:

\[x_I=\{x_{i},i\in I\},\ x_{-I}=\{x_{i},i\notin I\}\ \ \  \ \ \  \ I\subset K=\{1,2,\cdots,k\}\]

那么称这个分布为\textbf{满条件分布}。
\subsection{Gibbs采样原理}

当然M-H采样有接受分布的存在，因此效率还是不够高。Gibbs采样可以避免这个问题。吉布斯采样的基本原理是从满条件概率分布出发，从满条件概率分布中抽样，得到一个样本序列。基本原理是：吉布斯抽样过程是在一个马尔可夫链上随机游走，平稳分布就是目标联合分布。接下来我们介绍Gibbs采样的细节。

我们考虑二维情况：假设有一个二维分布\(p(x,y)\)，我们发现：

\[p(x_1,y_1)p(y_2|x_1)=p(x_1)p(y_1|x_1)p(y_2|x_1) \\
p(x_1,y_2)p(y_1|x_1)=p(x_1)p(y_2|x_1)p(y_1|x_1)\]

整理得：

\[p(x_1,y_1)p(y_2|x_1)=p(x_1,y_2)p(y_1|x_1)\]

假设点\(A\)为\((x_1,y_1)\)，点\(B\)为\((x_1,y_2)\)，我们可以改写为：

\[p(A)p(y_2|x_1)=p(B)p(y_1|x_1)\]

也就是说当点A转移到点B的时候服从上述的马尔可夫链。而上式的条件概率就是我们的转移矩阵或者转移核。也就是说对维度\(y\)的满条件分布即为上述马尔可夫链的转移核或者转移矩阵。如果把二维扩展到\(n\)维，即可以得到\(n\)维的吉布斯抽样。假设建议分布是当前变量\(x_j,\ j=1,2,\cdots,k\)（也就是抽取第\(j\)维变量）的满条件分布：\(q(x',x)=p(x'_j|x_{-j})\)，这里的\(x\)指的是当前的抽样，\(x'\)指的是下一步的抽样。扩展到维的情多维：

\[p(x'_j,x_{-j})p(x'_j|x_{-j})=p(x_j,x_{-j})p(x_j|x_{-j})\]

这时候的接受率\(\alpha=1\)。因此吉布斯采样可以认识是M-H采样的一种特殊情况。这个建议分布就是我们的目标分布的满条件分布。（这里和上一章的符号有所变化，不过内容是一样的，只是为了方便表达。）下面证明如何得到接受率\(\alpha=1\)：

根据M-H采样的接受分布公式，有：

\[q(x,x')=p(x'_j|x_{-j})\]

代入接受分布：

\[\alpha(x,x')
=min\left\{ 1,\frac{p(x')q(x',x)}{p(x)q(x,x')}  \right\}\]

因为我们是抽取当前变量\(j\)，因此有：
\begin{equation}
  \begin{aligned}
    \alpha(x,x')
    &=min\left\{ 1,\frac{p(x')q(x',x)}{p(x)q(x,x')}  \right\} \\
    &=min\left\{ 1,
    \frac{p(x'_j,x_{-j})p(x'_j|x_{-j})}
    {p(x_j,x_{-j})p(x_j|x_{-j})}  \right\}=1
    \end{aligned}
\end{equation}


之后抽取\(k\)维，循环\(n\)次，抛去燃烧期\(m\)，得到我们的抽样，计算均值。
\subsection{Gibbs采样算法}

\begin{figure}
\centering
%\includegraphics{http://cos.name/wp-content/uploads/2013/01/gibbs-algo-2.jpg}
\caption{}
\end{figure}

